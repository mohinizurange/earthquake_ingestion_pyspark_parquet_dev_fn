
##step 1 create cluster

gcloud dataproc clusters create dataproc-earthquake-cluster1234 \
--bucket earthquake-dp_temp_bk \
--enable-component-gateway \
--region us-central1 \
--zone us-central1-a \
--master-machine-type e2-standard-2 \
--master-boot-disk-size 100 \
--num-workers 2 \
--worker-machine-type e2-standard-2 \
--worker-boot-disk-size 100 \
--optional-components JUPYTER \
--image-version 2.0-debian10 \
--scopes https://www.googleapis.com/auth/cloud-platform  \
--tags pyspark \
--project spark-learning-43150 \
--initialization-actions gs://goog-dataproc-initialization-actions-us-central1/connectors/connectors.sh \
--metadata bigquery-connector-version=1.2.0 \
--metadata spark-bigquery-connector-version=0.21.0

or
--using jar new  ----
gcloud dataproc clusters create dataproc-earthquake-cluster1234 \
    --bucket earthquake-dp_temp_bk \
    --enable-component-gateway \
    --region us-central1 \
    --zone us-central1-a \
    --master-machine-type e2-standard-2 \
    --master-boot-disk-size 100 \
    --num-workers 2 \
    --worker-machine-type e2-standard-2 \
    --worker-boot-disk-size 100 \
    --optional-components JUPYTER \
    --image-version 2.0-debian10 \
    --scopes https://www.googleapis.com/auth/cloud-platform \
    --tags pyspark \
    --project spark-learning-43150 \
    --metadata bigquery-connector-version=1.2.0,spark-bigquery-connector-version=0.21.0 \
    --metadata JARS="https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar" \
    --metadata JARS="https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.24/postgresql-42.2.24.jar"


##job submit


### dynamic by passing url 

#### monthly
gcloud dataproc jobs submit pyspark \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/earthquake_pipeline_code_pyspark_fn.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1 \
-- \
--api_url "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.geojson" \
--pipeline_nm='monthly'

----or using jar
gcloud dataproc jobs submit pyspark  \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/earthquake_pipeline_code_pyspark_fn.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar \
--cluster=dataproc-cluster \
--region=us-central1 \
-- \
--api_url="https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson" \
--pipeline_nm='daily'



########## daily
gcloud dataproc jobs submit pyspark \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/earthquake_pipeline_code_pyspark_fn.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1 \
-- \
--api_url "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson" \
--pipeline_nm='daily'




#####################p1
gcloud dataproc jobs submit pyspark  \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/p1_extract_and_ingect_data_bronze.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1 \
-- \
--api_url="https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson" \
--pipeline_nm='daily'

###########p2
gcloud dataproc jobs submit pyspark  \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/p2_read_from_bronze_trans_write_silver.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1 \
-- \
--pipeline_nm='daily'

##########p3
gcloud dataproc jobs submit pyspark  \
gs://earthquake_analysis_buck/pysaprk/pyspark_code/p3_read_from_silver_write_bq_gold.py \
--py-files gs://earthquake_analysis_buck/pysaprk/pyspark_code/utility.py,gs://earthquake_analysis_buck/pysaprk/pyspark_code/config.py \
--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1 \
-- \
--pipeline_nm='daily'















































## for refrence 



##### for submit job to dataproc cluster for extract from api and upload in gcs
gcloud dataproc jobs submit pyspark \
gs://earthquake_analysis_buck/pyspark/pyspark_code/au.py \
--py-files gs://earthquake_analysis_buck/pyspark/pyspark_code/util.py \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1

##### for flatten data store in gsc and bigquery
gcloud dataproc jobs submit pyspark \
gs://earthquake_analysis_buck/pyspark/pyspark_code/earthquake_pipeline_code_historical_data.py \
--py-files gs://earthquake_analysis_buck/pyspark/pyspark_code/util.py \
--cluster=dataproc-earthquake-cluster1234 \
--region=us-central1


gcloud dataproc clusters create dataproc-earthquake-cluster1234 \  # Create a Dataproc cluster named 'dataproc-earthquake-cluster1234' for Spark jobs
    --bucket earthquake-dp_temp_bk \  # Use 'earthquake-dp_temp_bk' for storing intermediate data (e.g., data staging)
    --enable-component-gateway \  # Enable web access to Dataproc components like Jupyter for interactive development
    --region us-central1 \  # Set the region to 'us-central1' (location where the cluster will run)
    --zone us-central1-a \  # Set the availability zone within 'us-central1' for the cluster
    --master-machine-type e2-standard-2 \  # Set machine type for the master node to handle cluster management
    --master-boot-disk-size 100 \  # Allocate 100GB boot disk for the master node to store OS and software
    --num-workers 2 \  # Set the number of worker nodes (2) to run Spark tasks
    --worker-machine-type e2-standard-2 \  # Set machine type for worker nodes to handle data processing
    --worker-boot-disk-size 100 \  # Allocate 100GB boot disk for each worker node
    --optional-components JUPYTER \  # Install Jupyter for running interactive PySpark notebooks
    --image-version 2.0-debian10 \  # Use the Dataproc image with Debian 10 for compatibility with libraries
    --scopes https://www.googleapis.com/auth/cloud-platform \  # Allow cluster to access all GCP services (e.g., BigQuery, GCS)
    --tags pyspark \  # Tag the cluster for PySpark usage (useful for identification or resource management)
    --project spark-learning-43150 \  # Specify the Google Cloud project 'spark-learning-43150' for the cluster
    --metadata bigquery-connector-version=1.2.0,spark-bigquery-connector-version=0.21.0 \  # Set BigQuery connector versions for Spark-BigQuery integration
    --metadata JARS="https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.21.0.jar" \  # Add BigQuery connector JAR for Spark jobs
    --metadata JARS="https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.24/postgresql-42.2.24.jar"  # Add PostgreSQL JDBC JAR for connecting to PostgreSQL databases






